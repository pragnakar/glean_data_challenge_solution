# glean_data_challenge_solution

## Learning outcomes and Summary
-	I solved this challenge using Dask. I was eager to try it for the first time as I read that it is quite simple, and any code written using regular python can be tweaked easily to run using Dask. The design philosophy of Dask is advertised to reuse the interface of Pandas Data Frames and scikit learn to keep learning curve to minimum.  To bad, its just a good advertisement. While I was working, I realized not all functionality of Pandas data frame was implemented using Dask data frame. I was prototyping my solution using pandas frame and wanted to tweak it to work with Dask. This approach seems to be in efficient, I should have coded my solution to work with dask right from the start. I should have sticked to Spark 
-	My solution solved 3 out 4 logics which I was tasked to solve. I need to some more clarification to solve logic 4.
-	Though my current solution works fine, I realize it is suboptimal. It can be tweaked to make it more efficient. I am unable to implement to those tweaks as I am concerned that I may end up introducing more bugs to my solution before deadline than improving solution.
-	Datasets need to be larger than a certain threshold to really take advantage and get a feel of distributed frame works. The current dataset given is too small to take advantage of distributed frameworks. 
-	I could have done better If I had more time to play around with my solution. 
-	I believe the data set we are provided is a sample of what your company works with. I would suggest having a look at Apache Kafka, I read its is good tool for handling data streams and event driven solution. Please do have a look at it if you have not already. 
-	In conclusion, I enjoyed working on the challenge. Looking forward to hearing back from your team. I hope to meet you and your team members in round 2, and I will be presented with opportunity to explain on how I can add more value to your company. 
